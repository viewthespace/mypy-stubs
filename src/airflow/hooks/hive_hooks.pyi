from airflow.configuration import conf as conf
from airflow.exceptions import AirflowException as AirflowException
from airflow.hooks.base_hook import BaseHook as BaseHook
from airflow.utils.file import TemporaryDirectory as TemporaryDirectory
from airflow.utils.helpers import as_flattened_list as as_flattened_list
from airflow.utils.operator_helpers import AIRFLOW_VAR_NAME_FORMAT_MAPPING as AIRFLOW_VAR_NAME_FORMAT_MAPPING
from typing import Any

HIVE_QUEUE_PRIORITIES: Any

def get_context_from_env_var(): ...

class HiveCliHook(BaseHook):
    hive_cli_params: Any
    use_beeline: Any
    auth: Any
    conn: Any
    run_as: Any
    mapred_queue: Any
    mapred_queue_priority: Any
    mapred_job_name: Any
    def __init__(self, hive_cli_conn_id: str = ..., run_as: Any | None = ..., mapred_queue: Any | None = ..., mapred_queue_priority: Any | None = ..., mapred_job_name: Any | None = ...) -> None: ...
    sp: Any
    def run_cli(self, hql, schema: Any | None = ..., verbose: bool = ..., hive_conf: Any | None = ...): ...
    def test_hql(self, hql) -> None: ...
    def load_df(self, df, table, field_dict: Any | None = ..., delimiter: str = ..., encoding: str = ..., pandas_kwargs: Any | None = ..., **kwargs): ...
    def load_file(self, filepath, table, delimiter: str = ..., field_dict: Any | None = ..., create: bool = ..., overwrite: bool = ..., partition: Any | None = ..., recreate: bool = ..., tblproperties: Any | None = ...) -> None: ...
    def kill(self) -> None: ...

class HiveMetastoreHook(BaseHook):
    MAX_PART_COUNT: int
    conn_id: Any
    metastore: Any
    def __init__(self, metastore_conn_id: str = ...) -> None: ...
    def get_metastore_client(self): ...
    def get_conn(self): ...
    def check_for_partition(self, schema, table, partition): ...
    def check_for_named_partition(self, schema, table, partition_name): ...
    def get_table(self, table_name, db: str = ...): ...
    def get_tables(self, db, pattern: str = ...): ...
    def get_databases(self, pattern: str = ...): ...
    def get_partitions(self, schema, table_name, filter: Any | None = ...): ...
    def max_partition(self, schema, table_name, field: Any | None = ..., filter_map: Any | None = ...): ...
    def table_exists(self, table_name, db: str = ...): ...

class HiveServer2Hook(BaseHook):
    hiveserver2_conn_id: Any
    def __init__(self, hiveserver2_conn_id: str = ...) -> None: ...
    def get_conn(self, schema: Any | None = ...): ...
    def get_results(self, hql, schema: str = ..., fetch_size: Any | None = ..., hive_conf: Any | None = ...): ...
    def to_csv(self, hql, csv_filepath, schema: str = ..., delimiter: str = ..., lineterminator: str = ..., output_header: bool = ..., fetch_size: int = ..., hive_conf: Any | None = ...) -> None: ...
    def get_records(self, hql, schema: str = ...): ...
    def get_pandas_df(self, hql, schema: str = ...): ...

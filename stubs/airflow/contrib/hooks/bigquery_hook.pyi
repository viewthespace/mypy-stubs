from airflow import AirflowException as AirflowException
from airflow.contrib.hooks.gcp_api_base_hook import GoogleCloudBaseHook as GoogleCloudBaseHook
from airflow.hooks.dbapi_hook import DbApiHook as DbApiHook
from airflow.utils.log.logging_mixin import LoggingMixin as LoggingMixin
from pandas_gbq.gbq import GbqConnector
from typing import Any

log: Any

class BigQueryHook(GoogleCloudBaseHook, DbApiHook):
    conn_name_attr: str
    use_legacy_sql: Any
    location: Any
    def __init__(self, bigquery_conn_id: str = ..., delegate_to: Any | None = ..., use_legacy_sql: bool = ..., location: Any | None = ...) -> None: ...
    def get_conn(self): ...
    def get_service(self): ...
    def insert_rows(self, table, rows, target_fields: Any | None = ..., commit_every: int = ..., **kwargs) -> None: ...
    def get_pandas_df(self, sql, parameters: Any | None = ..., dialect: Any | None = ...): ...
    def table_exists(self, project_id, dataset_id, table_id): ...

class BigQueryPandasConnector(GbqConnector):
    project_id: Any
    reauth: Any
    service: Any
    verbose: Any
    dialect: Any
    def __init__(self, project_id, service, reauth: bool = ..., verbose: bool = ..., dialect: str = ...) -> None: ...

class BigQueryConnection:
    def __init__(self, *args, **kwargs) -> None: ...
    def close(self) -> None: ...
    def commit(self) -> None: ...
    def cursor(self): ...
    def rollback(self) -> None: ...

class BigQueryBaseCursor(LoggingMixin):
    service: Any
    project_id: Any
    use_legacy_sql: Any
    api_resource_configs: Any
    running_job_id: Any
    location: Any
    num_retries: Any
    def __init__(self, service, project_id, use_legacy_sql: bool = ..., api_resource_configs: Any | None = ..., location: Any | None = ..., num_retries: int = ...) -> None: ...
    def create_empty_table(self, project_id, dataset_id, table_id, schema_fields: Any | None = ..., time_partitioning: Any | None = ..., cluster_fields: Any | None = ..., labels: Any | None = ..., view: Any | None = ..., encryption_configuration: Any | None = ..., num_retries: int = ...) -> None: ...
    def create_external_table(self, external_project_dataset_table, schema_fields, source_uris, source_format: str = ..., autodetect: bool = ..., compression: str = ..., ignore_unknown_values: bool = ..., max_bad_records: int = ..., skip_leading_rows: int = ..., field_delimiter: str = ..., quote_character: Any | None = ..., allow_quoted_newlines: bool = ..., allow_jagged_rows: bool = ..., encoding: str = ..., src_fmt_configs: Any | None = ..., labels: Any | None = ..., encryption_configuration: Any | None = ...) -> None: ...
    def patch_table(self, dataset_id, table_id, project_id: Any | None = ..., description: Any | None = ..., expiration_time: Any | None = ..., external_data_configuration: Any | None = ..., friendly_name: Any | None = ..., labels: Any | None = ..., schema: Any | None = ..., time_partitioning: Any | None = ..., view: Any | None = ..., require_partition_filter: Any | None = ..., encryption_configuration: Any | None = ...) -> None: ...
    def run_query(self, bql: Any | None = ..., sql: Any | None = ..., destination_dataset_table: Any | None = ..., write_disposition: str = ..., allow_large_results: bool = ..., flatten_results: Any | None = ..., udf_config: Any | None = ..., use_legacy_sql: Any | None = ..., maximum_billing_tier: Any | None = ..., maximum_bytes_billed: Any | None = ..., create_disposition: str = ..., query_params: Any | None = ..., labels: Any | None = ..., schema_update_options: Any | None = ..., priority: str = ..., time_partitioning: Any | None = ..., api_resource_configs: Any | None = ..., cluster_fields: Any | None = ..., location: Any | None = ..., encryption_configuration: Any | None = ...): ...
    def run_extract(self, source_project_dataset_table, destination_cloud_storage_uris, compression: str = ..., export_format: str = ..., field_delimiter: str = ..., print_header: bool = ..., labels: Any | None = ...): ...
    def run_copy(self, source_project_dataset_tables, destination_project_dataset_table, write_disposition: str = ..., create_disposition: str = ..., labels: Any | None = ..., encryption_configuration: Any | None = ...): ...
    def run_load(self, destination_project_dataset_table, source_uris, schema_fields: Any | None = ..., source_format: str = ..., create_disposition: str = ..., skip_leading_rows: int = ..., write_disposition: str = ..., field_delimiter: str = ..., max_bad_records: int = ..., quote_character: Any | None = ..., ignore_unknown_values: bool = ..., allow_quoted_newlines: bool = ..., allow_jagged_rows: bool = ..., encoding: str = ..., schema_update_options: Any | None = ..., src_fmt_configs: Any | None = ..., time_partitioning: Any | None = ..., cluster_fields: Any | None = ..., autodetect: bool = ..., encryption_configuration: Any | None = ...): ...
    def run_with_configuration(self, configuration): ...
    def poll_job_complete(self, job_id): ...
    def cancel_query(self) -> None: ...
    def get_schema(self, dataset_id, table_id): ...
    def get_tabledata(self, dataset_id, table_id, max_results: Any | None = ..., selected_fields: Any | None = ..., page_token: Any | None = ..., start_index: Any | None = ...): ...
    def run_table_delete(self, deletion_dataset_table, ignore_if_missing: bool = ...) -> None: ...
    def run_table_upsert(self, dataset_id, table_resource, project_id: Any | None = ...): ...
    def run_grant_dataset_view_access(self, source_dataset, view_dataset, view_table, source_project: Any | None = ..., view_project: Any | None = ...): ...
    def create_empty_dataset(self, dataset_id: str = ..., project_id: str = ..., dataset_reference: Any | None = ...) -> None: ...
    def delete_dataset(self, project_id, dataset_id, delete_contents: bool = ...) -> None: ...
    def get_dataset(self, dataset_id, project_id: Any | None = ...): ...
    def get_datasets_list(self, project_id: Any | None = ...): ...
    def patch_dataset(self, dataset_id, dataset_resource, project_id: Any | None = ...): ...
    def update_dataset(self, dataset_id, dataset_resource, project_id: Any | None = ...): ...
    def insert_all(self, project_id, dataset_id, table_id, rows, ignore_unknown_values: bool = ..., skip_invalid_rows: bool = ..., fail_on_error: bool = ...) -> None: ...

class BigQueryCursor(BigQueryBaseCursor):
    buffersize: Any
    page_token: Any
    job_id: Any
    buffer: Any
    all_pages_loaded: bool
    def __init__(self, service, project_id, use_legacy_sql: bool = ..., location: Any | None = ..., num_retries: int = ...) -> None: ...
    @property
    def description(self) -> None: ...
    def close(self) -> None: ...
    @property
    def rowcount(self): ...
    def execute(self, operation, parameters: Any | None = ...) -> None: ...
    def executemany(self, operation, seq_of_parameters) -> None: ...
    def flush_results(self) -> None: ...
    def fetchone(self): ...
    def next(self): ...
    def fetchmany(self, size: Any | None = ...): ...
    def fetchall(self): ...
    def get_arraysize(self): ...
    def set_arraysize(self, arraysize) -> None: ...
    arraysize: Any
    def setinputsizes(self, sizes) -> None: ...
    def setoutputsize(self, size, column: Any | None = ...) -> None: ...
